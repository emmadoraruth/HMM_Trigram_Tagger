Emma Ziegellaub Eichler - edz2103@columbia.edu
COMS W4705 - Natural Language Processing
Programming Assignment 1 Report

There are six programs included in this assignment, all within a single file, nlp1.py. To execute any of these programs from the command line, simply type:
	python nlp1.py program_name [program_arguments].
The six programs are described below:

"rarify" which takes one argument, a file name for training data, and substitutes all words (not tags) appearing fewer than five times with the special symbol, "_RARE_"
For example: python nlp1.py rarify ner_train.dat

"tagger" takes two arguments-the name of a development file to be tagged and the file of tag counts generated by the training data. It uses emission counts to find the maximum probability tags and generates output in a file called "emission_predictions.txt".  It uses the counts for "_RARE_" for rare words.
For example: python nlp1.py tagger ner_dev.dat ner.counts

"trigram_est" takes a file of trigrams and the file of tag counts generates by training data and outputs the likelihood of each tag sequence into a file "trigram_predictions.txt". Since no sample file was provided, I have included one here (called "trigram_test.txt") with a very small number of examples for demonstration purposes.  Outputs to "trigram_estimates.txt"
For example: python nlp1.py trigram_est trigram_test.txt ner.counts

"viterbi" works identically to "tagger" except that it uses the Viterbi algorithm to estimate maximum likelihood tags instead of just emission counts. It writes its output to "viterbi_predictions.txt". It uses the counts for "_RARE_" for rare words.
For example: python nlp1.py viterbi ner_dev.dat ner.counts

"mod_rarify" works similarly to "rarify" except that it divides rare words into categories based on their attributes (categories described below).  Rare words that do not fall into any of the categories are replaced with the generic "_RARE_" symbol.
For example: python nlp1.py mod_rarify ner_train.dat

"mod_viterbi" works identically to "viterbi" except that it uses the counts for the rare word categories used by "mod_rarify" instead of just "_RARE_".
For example: python nlp1.py mod_viterbi ner_train.dat

Note that because "rarify" and "mod_rarify" both modify the training file, they should be run on clones of the same file. Counts should be regenerated by the script provided in the assignment whenever switching between "rarify" and "mod_rarfiy".

For my modified Viterbi algorithm (problem 6), I created six additional categories for rare words.  In order of precedence (greatest to least; that is, if a rare word matched more than one category, it was given the one highest on this list), they are:
	- "_NUM_": sequences of numeric digits, possibly with '$', '.', ',', '/', ':', and '-' characters, but no more than two consecutive special characters.  This category was chosen for its inclusion of dates, times, currency amounts, numeric adjectives, etcetera.  Numbers are always nouns or adjectives and are rarely named entities.
	- "_DOT_": any sequence of characters terminating in a period ('.').  This category was chosen for its inclusion of most abbreviations.  Abbreviations are often parts of names (middle initials, Inc., Co., etcetera) and are thus often named entities.
	- "_CAPS_": any sequence of only capital letters.  This category was chosen for its inclusion of acronyms and place abbreviations (such as states or NYC).  Such sequences are thus often named entities.
	- "_CAP_": any sequence beginning in a capital letter and followed by one or more lower case letters.  This category was chosen for its inclusion of proper nouns, which are named entities, although it unfortunately also includes unfamiliar words beginning sentences.
	- "_PUN_": any sequence consisting of only letters and one or more apostrophes and/or hyphens.  This category was chosen for its inclusion of hyphenated or apostrophized names, indicating named entities.
	- "_NORM_": any sequence consisting of only lowercase letters.  This represents a base case of an unknown word.
	- "_RARE_": the default case, should a rare word not match any of the others. This is to distinguish unknown dictionary words (which belong in "_NORM_") from unknown strings of characters.

The performance output of the emissions count tagger (problem 4) is as follows: 
	 precision 	recall 		F1-Score
Total:	 0.221858	0.525375	0.311974
PER:	 0.435451	0.231230	0.302061
ORG:	 0.475936	0.399103	0.434146
LOC:	 0.146791	0.869138	0.251162
MISC:	 0.517463	0.611292	0.560478
This tagger performs best on miscellaneous named entities.  However, although it has the best recall on location entities, it has terrible precision (15%) and and thus the worst F1-Score.  This means far more location entities are predicted than are correct but also that, due to this over-tagging, it correctly identifies close to the correct number of location entities (87%).  Overall, this model is only correct on 22% of the non-O tags it predicts, and only correct identifies 52% of non-O scores as such--in short, it performs slightly worse than random guessing in precision and only slightly better in recall, for a poor F1-Score of 31%.

The performance output of the Viterbi algorithm tagger using only "_RARE_" (problem 5) is as follows:
	 precision 	recall 		F1-Score
Total:	 0.775298	0.614905	0.685849
PER:	 0.762535	0.595756	0.668907
ORG:	 0.611855	0.478326	0.536913
LOC:	 0.876458	0.696292	0.776056
MISC:	 0.830065	0.689468	0.753262
This tagger performs drastically better than the emission counts one, more than doubling the F1-Score to 69%.  It correctly identifies 61% of non-O tags as such (a significant improvement over the emissions tagger), but of those that it identifies, it correctly categorizes 78% of them (3.5 times more accuracy than the emissions tagger). The greatest improvement is seen in the location entities category, which goes from having the worst precision to the best, at 88%.  Although the recall of location entities is deducted to 70%, it remains the highest of all categories.  The worst performance is now on organization entities, at 61% precision and 48% recall; otherwise, all named entities are above 75% precision and nearly 60% recall.  What this suggests is that categorizing rare words somewhat improves the named entities tagger's ability to distinguish between O and non-O tags, it drastically improves its ability to correctly categorize the non-O tags it identifies.

The performance output of the Viterbi algorithm tagger using categorized rare words (problem 6) is as follows:
	 precision 	recall 		F1-Score
Total:	 0.745351	0.729894	0.737542
PER:	 0.810674	0.785092	0.797678
ORG:	 0.543172	0.662930	0.597105
LOC:	 0.843101	0.752999	0.795507
MISC:	 0.816380	0.671010	0.736591
Because my categorizations identify sequences of characters likely to be named entities, it significantly increases the recall to 74%.  The recall of miscellaneous named entities drops 2% to 67%, but all the others increase significantly, with a minimum of 66% accuracy.  This is because the categories were chosen based on trends in the names of persons, organizations, and locations.  Precision in fact drops slightly overall by 3% to 75% (decreasing in two categories and increasing in two), because the chosen categories are often indicative of more than one type of named entity.  However, this decrease in precision is insignificant compared to the increase in recall, for a 5% overall increase in F1-Score.